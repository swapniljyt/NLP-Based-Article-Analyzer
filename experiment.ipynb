{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# Extraction of Article from the website\n",
    "def get_article_text(url):\n",
    "    # Send a GET request to the URL\n",
    "    res = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if res.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        html_page = res.content\n",
    "        soup = BeautifulSoup(html_page, features='html.parser')\n",
    "\n",
    "        # Find the article element\n",
    "        article_element = soup.find(\"article\")\n",
    "\n",
    "        if article_element:\n",
    "            # Extract all text within the article element and clean up spaces\n",
    "            article_text = ' '.join(article_element.stripped_strings)\n",
    "\n",
    "            # Return the cleaned article text\n",
    "            return article_text\n",
    "        else:\n",
    "            print(\"Article element not found.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the page. Status code: {res.status_code}\")\n",
    "        return None\n",
    "\n",
    "def url_to_dataframe(url):\n",
    "    # Get article text using the function\n",
    "    article_text = get_article_text(url)\n",
    "\n",
    "    if article_text:\n",
    "        # Create a DataFrame with a single column named 'Article'\n",
    "        df = pd.DataFrame(data={'Article': [article_text]})\n",
    "        return df\n",
    "    else:\n",
    "        # Return an empty DataFrame if article text is not available\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Example usage:https://insights.blackcoffer.com/in-future-or-in-upcoming-years-humans-and-machines-are-going-to-work-together-in-every-field-of-work/\n",
    "url = \"https://insights.blackcoffer.com/how-will-covid-19-affect-the-world-of-work-2/\"\n",
    "df = url_to_dataframe(url)\n",
    "\n",
    "# Print the DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Home Blackcoffer How will COVID-19 affect the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article\n",
       "0  Home Blackcoffer How will COVID-19 affect the ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "# Print the DataFrame\n",
    "display(df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Score: 49\n",
      "Negative Score: 63\n",
      "Polarity Score: -0.12499999888392858\n",
      "Subjectivity Score: 0.13429256578621995\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download NLTK data (you need to do this once)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define the directory paths\n",
    "stopwords_directory = '/root/src/NLP-Based-Article-Analyzer/StopWords'\n",
    "master_dictionary_directory = '/root/src/NLP-Based-Article-Analyzer/MasterDictionary'\n",
    "\n",
    "# Function to read the content of all files in a directory and store them in a list\n",
    "def read_stopwords_from_directory(directory):\n",
    "    stopword_strings = []\n",
    "    encodings = ['utf-8', 'latin-1', 'cp1252']  # List of possible encodings\n",
    "    for filename in os.listdir(directory):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        for encoding in encodings:\n",
    "            try:\n",
    "                with open(filepath, 'r', encoding=encoding) as file:\n",
    "                    stopword_strings.append(file.read())\n",
    "                break  # Break if the file is successfully read\n",
    "            except UnicodeDecodeError:\n",
    "                continue  # Try the next encoding\n",
    "    return stopword_strings\n",
    "\n",
    "# Read the stopwords from the directory\n",
    "stopword_strings = read_stopwords_from_directory(stopwords_directory)\n",
    "\n",
    "# Sentiment analysis function\n",
    "def calculate_sentiment_scores(dataframe):\n",
    "    # Define a regular expression pattern for detecting links\n",
    "    link_pattern = re.compile(r'https?://[^\\s]+|www\\.[^\\s]+')\n",
    "\n",
    "    # Initialize the final list of stopwords\n",
    "    final_stopword_list = []\n",
    "\n",
    "    # Loop through each stopword string\n",
    "    for stopword_string in stopword_strings:\n",
    "        # Convert to lowercase for consistency\n",
    "        stopword_string_lower = stopword_string.lower()\n",
    "\n",
    "        # Extract stopwords from the string and create a list\n",
    "        stopwords_from_string = stopword_string_lower.split()\n",
    "\n",
    "        # Filter out links and punctuations\n",
    "        filtered_words = [word for word in stopwords_from_string if not link_pattern.match(word) and not re.match(r'\\W+', word)]\n",
    "\n",
    "        # Extend the final_stopword_list with the filtered words\n",
    "        final_stopword_list.extend(filtered_words)\n",
    "\n",
    "    # Remove duplicates by converting the list to a set and then back to a list\n",
    "    final_stopword_list = list(set(final_stopword_list))\n",
    "\n",
    "    # Function to remove stop words\n",
    "    def remove_stopwords(text):\n",
    "        words = text.split()\n",
    "        filtered_words = [word for word in words if word.lower() not in final_stopword_list]\n",
    "        return ' '.join(filtered_words)\n",
    "\n",
    "    # Apply the remove_stopwords function to the 'Article' column\n",
    "    dataframe['Cleaned_Article'] = dataframe['Article'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "    # Read positive and negative words from the MasterDictionary directory\n",
    "    with open(os.path.join(master_dictionary_directory, 'positive-words.txt'), 'r', encoding='utf-8') as file:\n",
    "        positive_word_list = re.findall(r'\\b\\w+\\b', file.read())\n",
    "    \n",
    "    with open(os.path.join(master_dictionary_directory, 'negative-words.txt'), 'r', encoding='latin-1') as file:\n",
    "        negative_word_list = re.findall(r'\\b\\w+\\b', file.read())\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    df_words = word_tokenize(dataframe['Cleaned_Article'].iloc[0])\n",
    "\n",
    "    # Find words present in both df and positive_word_list\n",
    "    found_positive_word_list = [word for word in df_words if word in positive_word_list]\n",
    "\n",
    "    # Find words present in both df and negative_word_list\n",
    "    found_negative_word_list = [word for word in df_words if word in negative_word_list]\n",
    "\n",
    "    # Calculate Positive Score\n",
    "    positive_score = sum(1 for word in df_words if word in found_positive_word_list)\n",
    "\n",
    "    # Calculate Negative Score\n",
    "    negative_score = sum(-1 for word in df_words if word in found_negative_word_list) * -1\n",
    "\n",
    "    # Calculate Polarity Score\n",
    "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
    "\n",
    "    # Calculate Subjectivity Score\n",
    "    subjectivity_score = (positive_score + negative_score) / (len(df_words) + 0.000001)\n",
    "\n",
    "    return {\n",
    "        'Positive Score': positive_score,\n",
    "        'Negative Score': negative_score,\n",
    "        'Polarity Score': polarity_score,\n",
    "        'Subjectivity Score': subjectivity_score\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "df = url_to_dataframe(url)\n",
    "result = calculate_sentiment_scores(df)\n",
    "\n",
    "# Print the results\n",
    "for key, value in result.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Sentence Length: 40.6\n",
      "Percentage of Complex Words: 0.29064039408866993\n",
      "Fog Index: 16.356256157635467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "def calculate_sentence_complexity_metrics(dataframe):\n",
    "    # Tokenize the text into sentences and words\n",
    "    cleaned_articles = dataframe['Article'].apply(str)  # Ensure all entries are converted to strings\n",
    "    sentences = cleaned_articles.apply(sent_tokenize)\n",
    "    words = cleaned_articles.apply(word_tokenize)\n",
    "\n",
    "    # Calculate Average Sentence Length\n",
    "    average_sentence_length = words.apply(len) / sentences.apply(len)\n",
    "\n",
    "    # Calculate Percentage of Complex Words\n",
    "    complex_words = words.apply(lambda w: [word for word in w if len(word) > 6])  # Assuming words with more than 6 characters are complex\n",
    "    percentage_complex_words = complex_words.apply(len) / words.apply(len)\n",
    "\n",
    "    # Calculate Fog Index\n",
    "    fog_index = 0.4 * (average_sentence_length + percentage_complex_words)\n",
    "\n",
    "    return {\n",
    "        'Average Sentence Length': average_sentence_length.mean(),\n",
    "        'Percentage of Complex Words': percentage_complex_words.mean(),\n",
    "        'Fog Index': fog_index.mean()\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "sentence_complexity_metrics_result = calculate_sentence_complexity_metrics(df)\n",
    "\n",
    "# Print the results\n",
    "for key, value in sentence_complexity_metrics_result.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Number of Words Per Sentence: 40.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "def calculate_average_words_per_sentence(dataframe):\n",
    "    # Tokenize the text into sentences and words\n",
    "    sentences = [sent_tokenize(text) for text in dataframe['Article']]\n",
    "    words = [word_tokenize(text) for text in dataframe['Article']]\n",
    "\n",
    "    # Calculate Average Number of Words Per Sentence\n",
    "    average_words_per_sentence = len(words[0]) /len(sentences[0])\n",
    "    return average_words_per_sentence\n",
    "\n",
    "# Example usage:\n",
    "average_words_per_sentence_result = calculate_average_words_per_sentence(df)\n",
    "\n",
    "# Print the result\n",
    "print(\"Average Number of Words Per Sentence:\", average_words_per_sentence_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complex Word Count: 302\n"
     ]
    }
   ],
   "source": [
    "import syllables\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def calculate_complex_word_count(dataframe):\n",
    "    # Tokenize the text into words\n",
    "    words = [word_tokenize(text) for text in dataframe['Article']]\n",
    "\n",
    "    # Flatten the list of words\n",
    "    flattened_words = [word for sublist in words for word in sublist]\n",
    "\n",
    "    # Count complex words (words with more than two syllables)\n",
    "    complex_words_count = sum(1 for word in flattened_words if syllables.estimate(word) > 2)\n",
    "\n",
    "    return complex_words_count\n",
    "\n",
    "# Example usage:\n",
    "complex_words_count_result = calculate_complex_word_count(df)\n",
    "\n",
    "# Print the result\n",
    "print(\"Complex Word Count:\", complex_words_count_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Cleaned Words: 765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def calculate_total_cleaned_words(dataframe):\n",
    "    # Tokenize the text into words\n",
    "    words = [word_tokenize(text) for text in dataframe['Article']]\n",
    "\n",
    "    # Flatten the list of words\n",
    "    flattened_words = [word for sublist in words for word in sublist]\n",
    "\n",
    "    # Remove stop words and punctuations\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word.lower() for word in flattened_words if word.lower() not in stop_words and word.lower() not in string.punctuation]\n",
    "\n",
    "    # Remove additional punctuations from each word\n",
    "    filtered_words = [''.join(char for char in word if char.isalnum()) for word in filtered_words if any(char.isalnum() for char in word)]\n",
    "\n",
    "    # Count the total cleaned words\n",
    "    total_cleaned_words = len(filtered_words)\n",
    "\n",
    "    return total_cleaned_words\n",
    "\n",
    "# Example usage:\n",
    "total_cleaned_words_result = calculate_total_cleaned_words(df)\n",
    "\n",
    "# Print the result\n",
    "print(\"Total Cleaned Words:\", total_cleaned_words_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to count syllables in a word\n",
    "def count_syllables(word):\n",
    "    vowels = \"aeiouy\"\n",
    "    count = 0\n",
    "    prev_char = ''\n",
    "    for char in word:\n",
    "        char_lower = char.lower()\n",
    "        if char_lower in vowels and prev_char not in vowels:\n",
    "            count += 1\n",
    "        prev_char = char_lower\n",
    "    if word.endswith((\"es\", \"ed\")) and count > 1:\n",
    "        count -= 1  # Adjust for exceptions\n",
    "    return max(count, 1)  # At least one syllable\n",
    "\n",
    "def calculate_syllable_counts(dataframe):\n",
    "    # Tokenize the text into words\n",
    "    words = [word_tokenize(text) for text in dataframe['Article']]\n",
    "\n",
    "    # Flatten the list of words\n",
    "    flattened_words = [word for sublist in words for word in sublist]\n",
    "\n",
    "    # Count syllables per word\n",
    "    syllables_per_word = [count_syllables(word) for word in flattened_words]\n",
    "\n",
    "    # Calculate total syllables\n",
    "    total_syllables = sum(syllables_per_word)\n",
    "\n",
    "    # Calculate average syllables per word\n",
    "    average_syllables_per_word = total_syllables / len(flattened_words)\n",
    "\n",
    "\n",
    "    return syllables_per_word, total_syllables, average_syllables_per_word\n",
    "\n",
    "# Example usage:\n",
    "syllables_per_word, total_syllables, average_syllables_per_word = calculate_syllable_counts(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personal Pronouns Count: 5\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def count_personal_pronouns(dataframe):\n",
    "    # Define the regex pattern for personal pronouns\n",
    "    personal_pronouns_pattern = re.compile(r'\\b(?:I|we|my|ours|us)\\b', flags=re.IGNORECASE)\n",
    "\n",
    "    # Find matches in the text\n",
    "    personal_pronouns_matches = personal_pronouns_pattern.findall(dataframe['Article'].iloc[0])\n",
    "\n",
    "    # Count the occurrences\n",
    "    personal_pronouns_count = len(personal_pronouns_matches)\n",
    "\n",
    "    return personal_pronouns_count\n",
    "\n",
    "# Example usage:\n",
    "personal_pronouns_count_result = count_personal_pronouns(df)\n",
    "\n",
    "# Print the result\n",
    "print(\"Personal Pronouns Count:\", personal_pronouns_count_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Word Length: 4.836030964109782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "def calculate_average_word_length(dataframe):\n",
    "    # Tokenize the text into words\n",
    "    words = [word_tokenize(text) for text in dataframe['Article']]\n",
    "\n",
    "    # Flatten the list of words\n",
    "    flattened_words = [word for sublist in words for word in sublist]\n",
    "\n",
    "    # Calculate the sum of the total number of characters in each word\n",
    "    total_characters = sum(len(word) for word in flattened_words)\n",
    "\n",
    "    # Calculate the total number of words\n",
    "    total_words = len(flattened_words)\n",
    "\n",
    "    # Calculate the average word length\n",
    "    average_word_length = total_characters / total_words if total_words > 0 else 0\n",
    "\n",
    "    return average_word_length\n",
    "\n",
    "# Example usage:\n",
    "average_word_length_result = calculate_average_word_length(df)\n",
    "\n",
    "# Print the result\n",
    "print(\"Average Word Length:\", average_word_length_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
