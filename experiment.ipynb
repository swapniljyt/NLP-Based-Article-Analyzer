{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# Extraction of Article from the website\n",
    "def get_article_text(url):\n",
    "    # Send a GET request to the URL\n",
    "    res = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if res.status_code == 200:\n",
    "        # Parse the HTML content of the page\n",
    "        html_page = res.content\n",
    "        soup = BeautifulSoup(html_page, features='html.parser')\n",
    "\n",
    "        # Find the article element\n",
    "        article_element = soup.find(\"article\")\n",
    "\n",
    "        if article_element:\n",
    "            # Extract all text within the article element and clean up spaces\n",
    "            article_text = ' '.join(article_element.stripped_strings)\n",
    "\n",
    "            # Return the cleaned article text\n",
    "            return article_text\n",
    "        else:\n",
    "            print(\"Article element not found.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the page. Status code: {res.status_code}\")\n",
    "        return None\n",
    "\n",
    "def url_to_dataframe(url):\n",
    "    # Get article text using the function\n",
    "    article_text = get_article_text(url)\n",
    "\n",
    "    if article_text:\n",
    "        # Create a DataFrame with a single column named 'Article'\n",
    "        df = pd.DataFrame(data={'Article': [article_text]})\n",
    "        return df\n",
    "    else:\n",
    "        # Return an empty DataFrame if article text is not available\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Example usage:https://insights.blackcoffer.com/in-future-or-in-upcoming-years-humans-and-machines-are-going-to-work-together-in-every-field-of-work/\n",
    "url = \"https://insights.blackcoffer.com/how-will-covid-19-affect-the-world-of-work-2/\"\n",
    "df = url_to_dataframe(url)\n",
    "\n",
    "# Print the DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Home Blackcoffer How will COVID-19 affect the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article\n",
       "0  Home Blackcoffer How will COVID-19 affect the ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "# Print the DataFrame\n",
    "display(df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Score: 49\n",
      "Negative Score: 63\n",
      "Polarity Score: -0.12499999888392858\n",
      "Subjectivity Score: 0.13429256578621995\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download NLTK data (you need to do this once)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define the directory paths\n",
    "stopwords_directory = '/root/src/NLP-Based-Article-Analyzer/StopWords'\n",
    "master_dictionary_directory = '/root/src/NLP-Based-Article-Analyzer/MasterDictionary'\n",
    "\n",
    "# Function to read the content of all files in a directory and store them in a list\n",
    "def read_stopwords_from_directory(directory):\n",
    "    stopword_strings = []\n",
    "    encodings = ['utf-8', 'latin-1', 'cp1252']  # List of possible encodings\n",
    "    for filename in os.listdir(directory):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        for encoding in encodings:\n",
    "            try:\n",
    "                with open(filepath, 'r', encoding=encoding) as file:\n",
    "                    stopword_strings.append(file.read())\n",
    "                break  # Break if the file is successfully read\n",
    "            except UnicodeDecodeError:\n",
    "                continue  # Try the next encoding\n",
    "    return stopword_strings\n",
    "\n",
    "# Read the stopwords from the directory\n",
    "stopword_strings = read_stopwords_from_directory(stopwords_directory)\n",
    "\n",
    "# Sentiment analysis function\n",
    "def calculate_sentiment_scores(dataframe):\n",
    "    # Define a regular expression pattern for detecting links\n",
    "    link_pattern = re.compile(r'https?://[^\\s]+|www\\.[^\\s]+')\n",
    "\n",
    "    # Initialize the final list of stopwords\n",
    "    final_stopword_list = []\n",
    "\n",
    "    # Loop through each stopword string\n",
    "    for stopword_string in stopword_strings:\n",
    "        # Convert to lowercase for consistency\n",
    "        stopword_string_lower = stopword_string.lower()\n",
    "\n",
    "        # Extract stopwords from the string and create a list\n",
    "        stopwords_from_string = stopword_string_lower.split()\n",
    "\n",
    "        # Filter out links and punctuations\n",
    "        filtered_words = [word for word in stopwords_from_string if not link_pattern.match(word) and not re.match(r'\\W+', word)]\n",
    "\n",
    "        # Extend the final_stopword_list with the filtered words\n",
    "        final_stopword_list.extend(filtered_words)\n",
    "\n",
    "    # Remove duplicates by converting the list to a set and then back to a list\n",
    "    final_stopword_list = list(set(final_stopword_list))\n",
    "\n",
    "    # Function to remove stop words\n",
    "    def remove_stopwords(text):\n",
    "        words = text.split()\n",
    "        filtered_words = [word for word in words if word.lower() not in final_stopword_list]\n",
    "        return ' '.join(filtered_words)\n",
    "\n",
    "    # Apply the remove_stopwords function to the 'Article' column\n",
    "    dataframe['Cleaned_Article'] = dataframe['Article'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "    # Read positive and negative words from the MasterDictionary directory\n",
    "    with open(os.path.join(master_dictionary_directory, 'positive-words.txt'), 'r', encoding='utf-8') as file:\n",
    "        positive_word_list = re.findall(r'\\b\\w+\\b', file.read())\n",
    "    \n",
    "    with open(os.path.join(master_dictionary_directory, 'negative-words.txt'), 'r', encoding='latin-1') as file:\n",
    "        negative_word_list = re.findall(r'\\b\\w+\\b', file.read())\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    df_words = word_tokenize(dataframe['Cleaned_Article'].iloc[0])\n",
    "\n",
    "    # Find words present in both df and positive_word_list\n",
    "    found_positive_word_list = [word for word in df_words if word in positive_word_list]\n",
    "\n",
    "    # Find words present in both df and negative_word_list\n",
    "    found_negative_word_list = [word for word in df_words if word in negative_word_list]\n",
    "\n",
    "    # Calculate Positive Score\n",
    "    positive_score = sum(1 for word in df_words if word in found_positive_word_list)\n",
    "\n",
    "    # Calculate Negative Score\n",
    "    negative_score = sum(-1 for word in df_words if word in found_negative_word_list) * -1\n",
    "\n",
    "    # Calculate Polarity Score\n",
    "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
    "\n",
    "    # Calculate Subjectivity Score\n",
    "    subjectivity_score = (positive_score + negative_score) / (len(df_words) + 0.000001)\n",
    "\n",
    "    return {\n",
    "        'Positive Score': positive_score,\n",
    "        'Negative Score': negative_score,\n",
    "        'Polarity Score': polarity_score,\n",
    "        'Subjectivity Score': subjectivity_score\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "df = url_to_dataframe(url)\n",
    "result = calculate_sentiment_scores(df)\n",
    "\n",
    "# Print the results\n",
    "for key, value in result.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Sentence Length: 40.6\n",
      "Percentage of Complex Words: 0.29064039408866993\n",
      "Fog Index: 16.356256157635467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "def calculate_sentence_complexity_metrics(dataframe):\n",
    "    # Tokenize the text into sentences and words\n",
    "    cleaned_articles = dataframe['Article'].apply(str)  # Ensure all entries are converted to strings\n",
    "    sentences = cleaned_articles.apply(sent_tokenize)\n",
    "    words = cleaned_articles.apply(word_tokenize)\n",
    "\n",
    "    # Calculate Average Sentence Length\n",
    "    average_sentence_length = words.apply(len) / sentences.apply(len)\n",
    "\n",
    "    # Calculate Percentage of Complex Words\n",
    "    complex_words = words.apply(lambda w: [word for word in w if len(word) > 6])  # Assuming words with more than 6 characters are complex\n",
    "    percentage_complex_words = complex_words.apply(len) / words.apply(len)\n",
    "\n",
    "    # Calculate Fog Index\n",
    "    fog_index = 0.4 * (average_sentence_length + percentage_complex_words)\n",
    "\n",
    "    return {\n",
    "        'Average Sentence Length': average_sentence_length.mean(),\n",
    "        'Percentage of Complex Words': percentage_complex_words.mean(),\n",
    "        'Fog Index': fog_index.mean()\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "sentence_complexity_metrics_result = calculate_sentence_complexity_metrics(df)\n",
    "\n",
    "# Print the results\n",
    "for key, value in sentence_complexity_metrics_result.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Number of Words Per Sentence: 40.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "def calculate_average_words_per_sentence(dataframe):\n",
    "    # Tokenize the text into sentences and words\n",
    "    sentences = [sent_tokenize(text) for text in dataframe['Article']]\n",
    "    words = [word_tokenize(text) for text in dataframe['Article']]\n",
    "\n",
    "    # Calculate Average Number of Words Per Sentence\n",
    "    average_words_per_sentence = len(words[0]) /len(sentences[0])\n",
    "    return average_words_per_sentence\n",
    "\n",
    "# Example usage:\n",
    "average_words_per_sentence_result = calculate_average_words_per_sentence(df)\n",
    "\n",
    "# Print the result\n",
    "print(\"Average Number of Words Per Sentence:\", average_words_per_sentence_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complex Word Count: 302\n"
     ]
    }
   ],
   "source": [
    "import syllables\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def calculate_complex_word_count(dataframe):\n",
    "    # Tokenize the text into words\n",
    "    words = [word_tokenize(text) for text in dataframe['Article']]\n",
    "\n",
    "    # Flatten the list of words\n",
    "    flattened_words = [word for sublist in words for word in sublist]\n",
    "\n",
    "    # Count complex words (words with more than two syllables)\n",
    "    complex_words_count = sum(1 for word in flattened_words if syllables.estimate(word) > 2)\n",
    "\n",
    "    return complex_words_count\n",
    "\n",
    "# Example usage:\n",
    "complex_words_count_result = calculate_complex_word_count(df)\n",
    "\n",
    "# Print the result\n",
    "print(\"Complex Word Count:\", complex_words_count_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Cleaned Words: 765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def calculate_total_cleaned_words(dataframe):\n",
    "    # Tokenize the text into words\n",
    "    words = [word_tokenize(text) for text in dataframe['Article']]\n",
    "\n",
    "    # Flatten the list of words\n",
    "    flattened_words = [word for sublist in words for word in sublist]\n",
    "\n",
    "    # Remove stop words and punctuations\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word.lower() for word in flattened_words if word.lower() not in stop_words and word.lower() not in string.punctuation]\n",
    "\n",
    "    # Remove additional punctuations from each word\n",
    "    filtered_words = [''.join(char for char in word if char.isalnum()) for word in filtered_words if any(char.isalnum() for char in word)]\n",
    "\n",
    "    # Count the total cleaned words\n",
    "    total_cleaned_words = len(filtered_words)\n",
    "\n",
    "    return total_cleaned_words\n",
    "\n",
    "# Example usage:\n",
    "total_cleaned_words_result = calculate_total_cleaned_words(df)\n",
    "\n",
    "# Print the result\n",
    "print(\"Total Cleaned Words:\", total_cleaned_words_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to count syllables in a word\n",
    "def count_syllables(word):\n",
    "    vowels = \"aeiouy\"\n",
    "    count = 0\n",
    "    prev_char = ''\n",
    "    for char in word:\n",
    "        char_lower = char.lower()\n",
    "        if char_lower in vowels and prev_char not in vowels:\n",
    "            count += 1\n",
    "        prev_char = char_lower\n",
    "    if word.endswith((\"es\", \"ed\")) and count > 1:\n",
    "        count -= 1  # Adjust for exceptions\n",
    "    return max(count, 1)  # At least one syllable\n",
    "\n",
    "def calculate_syllable_counts(dataframe):\n",
    "    # Tokenize the text into words\n",
    "    words = [word_tokenize(text) for text in dataframe['Article']]\n",
    "\n",
    "    # Flatten the list of words\n",
    "    flattened_words = [word for sublist in words for word in sublist]\n",
    "\n",
    "    # Count syllables per word\n",
    "    syllables_per_word = [count_syllables(word) for word in flattened_words]\n",
    "\n",
    "    # Calculate total syllables\n",
    "    total_syllables = sum(syllables_per_word)\n",
    "\n",
    "    # Calculate average syllables per word\n",
    "    average_syllables_per_word = total_syllables / len(flattened_words)\n",
    "\n",
    "\n",
    "    return syllables_per_word, total_syllables, average_syllables_per_word\n",
    "\n",
    "# Example usage:\n",
    "syllables_per_word, total_syllables, average_syllables_per_word = calculate_syllable_counts(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personal Pronouns Count: 5\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def count_personal_pronouns(dataframe):\n",
    "    # Define the regex pattern for personal pronouns\n",
    "    personal_pronouns_pattern = re.compile(r'\\b(?:I|we|my|ours|us)\\b', flags=re.IGNORECASE)\n",
    "\n",
    "    # Find matches in the text\n",
    "    personal_pronouns_matches = personal_pronouns_pattern.findall(dataframe['Article'].iloc[0])\n",
    "\n",
    "    # Count the occurrences\n",
    "    personal_pronouns_count = len(personal_pronouns_matches)\n",
    "\n",
    "    return personal_pronouns_count\n",
    "\n",
    "# Example usage:\n",
    "personal_pronouns_count_result = count_personal_pronouns(df)\n",
    "\n",
    "# Print the result\n",
    "print(\"Personal Pronouns Count:\", personal_pronouns_count_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Word Length: 4.836030964109782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "def calculate_average_word_length(dataframe):\n",
    "    # Tokenize the text into words\n",
    "    words = [word_tokenize(text) for text in dataframe['Article']]\n",
    "\n",
    "    # Flatten the list of words\n",
    "    flattened_words = [word for sublist in words for word in sublist]\n",
    "\n",
    "    # Calculate the sum of the total number of characters in each word\n",
    "    total_characters = sum(len(word) for word in flattened_words)\n",
    "\n",
    "    # Calculate the total number of words\n",
    "    total_words = len(flattened_words)\n",
    "\n",
    "    # Calculate the average word length\n",
    "    average_word_length = total_characters / total_words if total_words > 0 else 0\n",
    "\n",
    "    return average_word_length\n",
    "\n",
    "# Example usage:\n",
    "average_word_length_result = calculate_average_word_length(df)\n",
    "\n",
    "# Print the result\n",
    "print(\"Average Word Length:\", average_word_length_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(       URL_ID                                                URL\n",
       " 0  bctech2011  https://insights.blackcoffer.com/ml-and-ai-bas...\n",
       " 1  bctech2012  https://insights.blackcoffer.com/streamlined-i...\n",
       " 2  bctech2013  https://insights.blackcoffer.com/efficient-dat...\n",
       " 3  bctech2014  https://insights.blackcoffer.com/effective-man...\n",
       " 4  bctech2015  https://insights.blackcoffer.com/streamlined-t...,\n",
       "        URL_ID                                                URL  \\\n",
       " 0  bctech2011  https://insights.blackcoffer.com/ml-and-ai-bas...   \n",
       " 1  bctech2012  https://insights.blackcoffer.com/streamlined-i...   \n",
       " 2  bctech2013  https://insights.blackcoffer.com/efficient-dat...   \n",
       " 3  bctech2014  https://insights.blackcoffer.com/effective-man...   \n",
       " 4  bctech2015  https://insights.blackcoffer.com/streamlined-t...   \n",
       " \n",
       "    POSITIVE SCORE NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  \\\n",
       " 0             NaN            NaN             NaN                 NaN   \n",
       " 1             NaN            NaN             NaN                 NaN   \n",
       " 2             NaN            NaN             NaN                 NaN   \n",
       " 3             NaN            NaN             NaN                 NaN   \n",
       " 4             NaN                            NaN                 NaN   \n",
       " \n",
       "    AVG SENTENCE LENGTH  PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
       " 0                  NaN                          NaN        NaN   \n",
       " 1                  NaN                          NaN        NaN   \n",
       " 2                  NaN                          NaN        NaN   \n",
       " 3                  NaN                          NaN        NaN   \n",
       " 4                  NaN                          NaN        NaN   \n",
       " \n",
       "    AVG NUMBER OF WORDS PER SENTENCE  COMPLEX WORD COUNT  WORD COUNT  \\\n",
       " 0                               NaN                 NaN         NaN   \n",
       " 1                               NaN                 NaN         NaN   \n",
       " 2                               NaN                 NaN         NaN   \n",
       " 3                               NaN                 NaN         NaN   \n",
       " 4                               NaN                 NaN         NaN   \n",
       " \n",
       "    SYLLABLE PER WORD  PERSONAL PRONOUNS  AVG WORD LENGTH  \n",
       " 0                NaN                NaN              NaN  \n",
       " 1                NaN                NaN              NaN  \n",
       " 2                NaN                NaN              NaN  \n",
       " 3                NaN                NaN              NaN  \n",
       " 4                NaN                NaN              NaN  )"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the input and output CSV files\n",
    "input_file_path = '/root/src/NLP-Based-Article-Analyzer/Input.csv'\n",
    "output_file_path = '/root/src/NLP-Based-Article-Analyzer/Output Data Structure.csv'\n",
    "\n",
    "# Read the URLs from the input CSV file\n",
    "input_df = pd.read_csv(input_file_path)\n",
    "\n",
    "# Read the output data structure\n",
    "output_df = pd.read_csv(output_file_path)\n",
    "\n",
    "# Display the first few rows of each DataFrame\n",
    "input_df.head(), output_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.URL DECODING........\n",
      "2.URL DECODING........\n",
      "3.URL DECODING........\n",
      "4.URL DECODING........\n",
      "5.URL DECODING........\n",
      "6.URL DECODING........\n",
      "7.URL DECODING........\n",
      "8.URL DECODING........\n",
      "9.URL DECODING........\n",
      "10.URL DECODING........\n",
      "11.URL DECODING........\n",
      "12.URL DECODING........\n",
      "13.URL DECODING........\n",
      "14.URL DECODING........\n",
      "15.URL DECODING........\n",
      "16.URL DECODING........\n",
      "17.URL DECODING........\n",
      "18.URL DECODING........\n",
      "19.URL DECODING........\n",
      "20.URL DECODING........\n",
      "21.URL DECODING........\n",
      "22.URL DECODING........\n",
      "23.URL DECODING........\n",
      "24.URL DECODING........\n",
      "25.URL DECODING........\n",
      "26.URL DECODING........\n",
      "27.URL DECODING........\n",
      "28.URL DECODING........\n",
      "29.URL DECODING........\n",
      "30.URL DECODING........\n",
      "31.URL DECODING........\n",
      "32.URL DECODING........\n",
      "33.URL DECODING........\n",
      "34.URL DECODING........\n",
      "35.URL DECODING........\n",
      "36.URL DECODING........\n",
      "37.URL DECODING........\n",
      "38.URL DECODING........\n",
      "39.URL DECODING........\n",
      "40.URL DECODING........\n",
      "41.URL DECODING........\n",
      "42.URL DECODING........\n",
      "43.URL DECODING........\n",
      "44.URL DECODING........\n",
      "45.URL DECODING........\n",
      "46.URL DECODING........\n",
      "47.URL DECODING........\n",
      "48.URL DECODING........\n",
      "49.URL DECODING........\n",
      "50.URL DECODING........\n",
      "51.URL DECODING........\n",
      "52.URL DECODING........\n",
      "53.URL DECODING........\n",
      "54.URL DECODING........\n",
      "55.URL DECODING........\n",
      "56.URL DECODING........\n",
      "57.URL DECODING........\n",
      "58.URL DECODING........\n",
      "59.URL DECODING........\n",
      "60.URL DECODING........\n",
      "61.URL DECODING........\n",
      "62.URL DECODING........\n",
      "63.URL DECODING........\n",
      "64.URL DECODING........\n",
      "65.URL DECODING........\n",
      "66.URL DECODING........\n",
      "67.URL DECODING........\n",
      "68.URL DECODING........\n",
      "69.URL DECODING........\n",
      "70.URL DECODING........\n",
      "71.URL DECODING........\n",
      "72.URL DECODING........\n",
      "73.URL DECODING........\n",
      "74.URL DECODING........\n",
      "75.URL DECODING........\n",
      "76.URL DECODING........\n",
      "77.URL DECODING........\n",
      "78.URL DECODING........\n",
      "79.URL DECODING........\n",
      "80.URL DECODING........\n",
      "81.URL DECODING........\n",
      "82.URL DECODING........\n",
      "83.URL DECODING........\n",
      "84.URL DECODING........\n",
      "85.URL DECODING........\n",
      "86.URL DECODING........\n",
      "87.URL DECODING........\n",
      "88.URL DECODING........\n",
      "89.URL DECODING........\n",
      "90.URL DECODING........\n",
      "91.URL DECODING........\n",
      "92.URL DECODING........\n",
      "93.URL DECODING........\n",
      "94.URL DECODING........\n",
      "95.URL DECODING........\n",
      "96.URL DECODING........\n",
      "97.URL DECODING........\n",
      "98.URL DECODING........\n",
      "99.URL DECODING........\n",
      "100.URL DECODING........\n",
      "101.URL DECODING........\n",
      "102.URL DECODING........\n",
      "103.URL DECODING........\n",
      "104.URL DECODING........\n",
      "105.URL DECODING........\n",
      "106.URL DECODING........\n",
      "107.URL DECODING........\n",
      "108.URL DECODING........\n",
      "109.URL DECODING........\n",
      "110.URL DECODING........\n",
      "111.URL DECODING........\n",
      "112.URL DECODING........\n",
      "113.URL DECODING........\n",
      "114.URL DECODING........\n",
      "115.URL DECODING........\n",
      "116.URL DECODING........\n",
      "117.URL DECODING........\n",
      "118.URL DECODING........\n",
      "119.URL DECODING........\n",
      "120.URL DECODING........\n",
      "121.URL DECODING........\n",
      "122.URL DECODING........\n",
      "123.URL DECODING........\n",
      "124.URL DECODING........\n",
      "125.URL DECODING........\n",
      "126.URL DECODING........\n",
      "127.URL DECODING........\n",
      "128.URL DECODING........\n",
      "129.URL DECODING........\n",
      "130.URL DECODING........\n",
      "131.URL DECODING........\n",
      "132.URL DECODING........\n",
      "133.URL DECODING........\n",
      "134.URL DECODING........\n",
      "135.URL DECODING........\n",
      "136.URL DECODING........\n",
      "137.URL DECODING........\n",
      "138.URL DECODING........\n",
      "139.URL DECODING........\n",
      "140.URL DECODING........\n",
      "141.URL DECODING........\n",
      "142.URL DECODING........\n",
      "143.URL DECODING........\n",
      "144.URL DECODING........\n",
      "145.URL DECODING........\n",
      "146.URL DECODING........\n",
      "147.URL DECODING........\n",
      "Parameter Updated in Dataframe Sucessfully....\n"
     ]
    }
   ],
   "source": [
    "# Assuming your DataFrame is named output_df\n",
    "# Iterate over each row in the output_df DataFrame\n",
    "n=0\n",
    "for index, row in output_df.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "\n",
    "    # Use your function to convert the URL to DataFrame (replace url_to_dataframe with your actual function)\n",
    "    df = url_to_dataframe(url)\n",
    "\n",
    "    # Calculate sentiment scores\n",
    "    sentiment_scores = calculate_sentiment_scores(df)\n",
    "\n",
    "    # Calculate additional parameters\n",
    "    sentence_complexity_metrics = calculate_sentence_complexity_metrics(df)\n",
    "    avg_words_per_sentence = calculate_average_words_per_sentence(df)\n",
    "    complex_word_count = calculate_complex_word_count(df)\n",
    "    total_cleaned_words = calculate_total_cleaned_words(df)\n",
    "    syllables_per_word_result, total_syllables_result, average_syllables_per_word_result = calculate_syllable_counts(df)\n",
    "    personal_pronouns_count = count_personal_pronouns(df)\n",
    "    avg_word_length = calculate_average_word_length(df)\n",
    "\n",
    "    # Update the corresponding row in output_df with sentiment scores and additional parameters\n",
    "    output_df.at[index, 'POSITIVE SCORE'] = sentiment_scores['Positive Score']\n",
    "    output_df.at[index, 'NEGATIVE SCORE'] = sentiment_scores['Negative Score']\n",
    "    output_df.at[index, 'POLARITY SCORE'] = sentiment_scores['Polarity Score']\n",
    "    output_df.at[index, 'SUBJECTIVITY SCORE'] = sentiment_scores['Subjectivity Score']\n",
    "\n",
    "    # Update with additional parameters\n",
    "    output_df.at[index, 'AVG SENTENCE LENGTH'] = sentence_complexity_metrics['Average Sentence Length']\n",
    "    output_df.at[index, 'PERCENTAGE OF COMPLEX WORDS'] = sentence_complexity_metrics['Percentage of Complex Words']\n",
    "    output_df.at[index, 'FOG INDEX'] = sentence_complexity_metrics['Fog Index']\n",
    "    output_df.at[index, 'AVG NUMBER OF WORDS PER SENTENCE'] = avg_words_per_sentence\n",
    "    output_df.at[index, 'COMPLEX WORD COUNT'] = complex_word_count\n",
    "    output_df.at[index, 'WORD COUNT'] = total_cleaned_words\n",
    "    output_df.at[index, 'SYLLABLE PER WORD'] = average_syllables_per_word_result\n",
    "    output_df.at[index, 'PERSONAL PRONOUNS'] = personal_pronouns_count\n",
    "    output_df.at[index, 'AVG WORD LENGTH'] = avg_word_length\n",
    "    n=n+1\n",
    "    print(f\"{n}.URL DECODING........\")\n",
    "\n",
    "print(\"Parameter Updated in Dataframe Sucessfully....\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bctech2011</td>\n",
       "      <td>https://insights.blackcoffer.com/ml-and-ai-bas...</td>\n",
       "      <td>107.0</td>\n",
       "      <td>45</td>\n",
       "      <td>0.407895</td>\n",
       "      <td>0.062372</td>\n",
       "      <td>18.206704</td>\n",
       "      <td>0.424670</td>\n",
       "      <td>7.452550</td>\n",
       "      <td>18.206704</td>\n",
       "      <td>1109.0</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>1.865296</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.523167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bctech2012</td>\n",
       "      <td>https://insights.blackcoffer.com/streamlined-i...</td>\n",
       "      <td>17.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.036728</td>\n",
       "      <td>14.826923</td>\n",
       "      <td>0.429313</td>\n",
       "      <td>6.102494</td>\n",
       "      <td>14.826923</td>\n",
       "      <td>241.0</td>\n",
       "      <td>529.0</td>\n",
       "      <td>1.926070</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.884565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bctech2013</td>\n",
       "      <td>https://insights.blackcoffer.com/efficient-dat...</td>\n",
       "      <td>25.0</td>\n",
       "      <td>11</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.051799</td>\n",
       "      <td>25.800000</td>\n",
       "      <td>0.379845</td>\n",
       "      <td>10.471938</td>\n",
       "      <td>25.800000</td>\n",
       "      <td>245.0</td>\n",
       "      <td>603.0</td>\n",
       "      <td>1.758583</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.511628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bctech2014</td>\n",
       "      <td>https://insights.blackcoffer.com/effective-man...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15</td>\n",
       "      <td>-0.034483</td>\n",
       "      <td>0.049153</td>\n",
       "      <td>13.618182</td>\n",
       "      <td>0.405874</td>\n",
       "      <td>5.609623</td>\n",
       "      <td>13.618182</td>\n",
       "      <td>223.0</td>\n",
       "      <td>521.0</td>\n",
       "      <td>1.843792</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.708945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bctech2015</td>\n",
       "      <td>https://insights.blackcoffer.com/streamlined-t...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>27.687500</td>\n",
       "      <td>0.365688</td>\n",
       "      <td>11.221275</td>\n",
       "      <td>27.687500</td>\n",
       "      <td>239.0</td>\n",
       "      <td>543.0</td>\n",
       "      <td>1.753950</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.232506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>bctech2153</td>\n",
       "      <td>https://insights.blackcoffer.com/population-an...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>19</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.039702</td>\n",
       "      <td>33.222222</td>\n",
       "      <td>0.303512</td>\n",
       "      <td>13.410294</td>\n",
       "      <td>33.222222</td>\n",
       "      <td>288.0</td>\n",
       "      <td>773.0</td>\n",
       "      <td>1.695652</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.005017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>bctech2154</td>\n",
       "      <td>https://insights.blackcoffer.com/google-lsa-ap...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>21</td>\n",
       "      <td>-0.076923</td>\n",
       "      <td>0.035009</td>\n",
       "      <td>25.791045</td>\n",
       "      <td>0.286458</td>\n",
       "      <td>10.431001</td>\n",
       "      <td>25.791045</td>\n",
       "      <td>370.0</td>\n",
       "      <td>1041.0</td>\n",
       "      <td>1.656250</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.918403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>bctech2155</td>\n",
       "      <td>https://insights.blackcoffer.com/healthcare-da...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12</td>\n",
       "      <td>-0.142857</td>\n",
       "      <td>0.055851</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>0.248804</td>\n",
       "      <td>13.299522</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>106.0</td>\n",
       "      <td>372.0</td>\n",
       "      <td>1.570973</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.751196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>bctech2156</td>\n",
       "      <td>https://insights.blackcoffer.com/budget-sales-...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.999999</td>\n",
       "      <td>0.003802</td>\n",
       "      <td>339.000000</td>\n",
       "      <td>0.289086</td>\n",
       "      <td>135.715634</td>\n",
       "      <td>339.000000</td>\n",
       "      <td>72.0</td>\n",
       "      <td>251.0</td>\n",
       "      <td>1.705015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.138643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>bctech2157</td>\n",
       "      <td>https://insights.blackcoffer.com/amazon-buy-bo...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018605</td>\n",
       "      <td>36.555556</td>\n",
       "      <td>0.273556</td>\n",
       "      <td>14.731645</td>\n",
       "      <td>36.555556</td>\n",
       "      <td>66.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>1.589666</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.808511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         URL_ID                                                URL  \\\n",
       "0    bctech2011  https://insights.blackcoffer.com/ml-and-ai-bas...   \n",
       "1    bctech2012  https://insights.blackcoffer.com/streamlined-i...   \n",
       "2    bctech2013  https://insights.blackcoffer.com/efficient-dat...   \n",
       "3    bctech2014  https://insights.blackcoffer.com/effective-man...   \n",
       "4    bctech2015  https://insights.blackcoffer.com/streamlined-t...   \n",
       "..          ...                                                ...   \n",
       "142  bctech2153  https://insights.blackcoffer.com/population-an...   \n",
       "143  bctech2154  https://insights.blackcoffer.com/google-lsa-ap...   \n",
       "144  bctech2155  https://insights.blackcoffer.com/healthcare-da...   \n",
       "145  bctech2156  https://insights.blackcoffer.com/budget-sales-...   \n",
       "146  bctech2157  https://insights.blackcoffer.com/amazon-buy-bo...   \n",
       "\n",
       "     POSITIVE SCORE NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  \\\n",
       "0             107.0             45        0.407895            0.062372   \n",
       "1              17.0              5        0.545455            0.036728   \n",
       "2              25.0             11        0.388889            0.051799   \n",
       "3              14.0             15       -0.034483            0.049153   \n",
       "4              10.0              5        0.333333            0.023810   \n",
       "..              ...            ...             ...                 ...   \n",
       "142            13.0             19       -0.187500            0.039702   \n",
       "143            18.0             21       -0.076923            0.035009   \n",
       "144             9.0             12       -0.142857            0.055851   \n",
       "145             0.0              1       -0.999999            0.003802   \n",
       "146             2.0              2        0.000000            0.018605   \n",
       "\n",
       "     AVG SENTENCE LENGTH  PERCENTAGE OF COMPLEX WORDS   FOG INDEX  \\\n",
       "0              18.206704                     0.424670    7.452550   \n",
       "1              14.826923                     0.429313    6.102494   \n",
       "2              25.800000                     0.379845   10.471938   \n",
       "3              13.618182                     0.405874    5.609623   \n",
       "4              27.687500                     0.365688   11.221275   \n",
       "..                   ...                          ...         ...   \n",
       "142            33.222222                     0.303512   13.410294   \n",
       "143            25.791045                     0.286458   10.431001   \n",
       "144            33.000000                     0.248804   13.299522   \n",
       "145           339.000000                     0.289086  135.715634   \n",
       "146            36.555556                     0.273556   14.731645   \n",
       "\n",
       "     AVG NUMBER OF WORDS PER SENTENCE  COMPLEX WORD COUNT  WORD COUNT  \\\n",
       "0                           18.206704              1109.0      2014.0   \n",
       "1                           14.826923               241.0       529.0   \n",
       "2                           25.800000               245.0       603.0   \n",
       "3                           13.618182               223.0       521.0   \n",
       "4                           27.687500               239.0       543.0   \n",
       "..                                ...                 ...         ...   \n",
       "142                         33.222222               288.0       773.0   \n",
       "143                         25.791045               370.0      1041.0   \n",
       "144                         33.000000               106.0       372.0   \n",
       "145                        339.000000                72.0       251.0   \n",
       "146                         36.555556                66.0       212.0   \n",
       "\n",
       "     SYLLABLE PER WORD  PERSONAL PRONOUNS  AVG WORD LENGTH  \n",
       "0             1.865296                3.0         5.523167  \n",
       "1             1.926070                1.0         5.884565  \n",
       "2             1.758583                1.0         5.511628  \n",
       "3             1.843792                1.0         5.708945  \n",
       "4             1.753950                3.0         5.232506  \n",
       "..                 ...                ...              ...  \n",
       "142           1.695652                4.0         5.005017  \n",
       "143           1.656250                7.0         4.918403  \n",
       "144           1.570973               14.0         4.751196  \n",
       "145           1.705015                0.0         5.138643  \n",
       "146           1.589666                1.0         4.808511  \n",
       "\n",
       "[147 rows x 15 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
